{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama Index Framework\n",
    "This notebook is dedicated to learning about the Llama Index Framework for creating Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Make sure to setup your environment currectly \n",
    "# For this project I used python 3.12.3 \n",
    "\n",
    "# Since we are using huggingface models, we need to install the huggingface_hub library\n",
    "%pip install -q llama-index-llms-huggingface-api llama-index-embeddings-huggingface\n",
    "%pip install -q lmstudio\n",
    "%pip install -q load-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "LMStudioWebsocketError",
     "evalue": "\n    LM Studio is not reachable at ws://localhost:1234/system (due to httpx.ConnectError: All connection attempts failed).\n    Is LM Studio running?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLMStudioWebsocketError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlmstudio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlms\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#List all model avialable locally\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m lmstudio_downloaded_models = \u001b[43mlms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlist_downloaded_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m lmstudio_downloaded_models:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownloaded model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mLMStudioWebsocketError\u001b[39m: \n    LM Studio is not reachable at ws://localhost:1234/system (due to httpx.ConnectError: All connection attempts failed).\n    Is LM Studio running?"
     ]
    }
   ],
   "source": [
    "# Test out llm studio sdk\n",
    "import lmstudio as lms\n",
    "\n",
    "#List all model avialable locally\n",
    "lmstudio_downloaded_models = lms.list_downloaded_models()\n",
    "for model in lmstudio_downloaded_models:\n",
    "    print(f\"Downloaded model: {model}\")\n",
    "    \n",
    "# List LLM's only\n",
    "lmstudio_llms_only = lms.list_downloaded_models(\"llm\")\n",
    "for llm in lmstudio_llms_only:\n",
    "    print(f\"LLM: {llm}\")\n",
    "    \n",
    "# List embeddings only\n",
    "lmstudio_embeddings_only = lms.list_downloaded_models(\"embedding\")\n",
    "for embedding in lmstudio_embeddings_only:\n",
    "    print(f\"Embedding: {embedding}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmstudio as lms\n",
    "\n",
    "lmstudio_llm = 78 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "LMStudioWebsocketError",
     "evalue": "\n    LM Studio is not reachable at ws://localhost:1234/llm (due to httpx.ConnectError: All connection attempts failed).\n    Is LM Studio running?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLMStudioWebsocketError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m     year: \u001b[38;5;28mint\u001b[39m\n\u001b[32m      9\u001b[39m     rating: \u001b[38;5;28mfloat\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m lmstudio_llm = \u001b[43mlms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Gets the currrent loaded model\u001b[39;00m\n\u001b[32m     13\u001b[39m prompt = \u001b[33m\"\u001b[39m\u001b[33mTell me about the book \u001b[39m\u001b[33m'\u001b[39m\u001b[33mThe Great Gatsby\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m response = lmstudio_llm.respond(\n\u001b[32m     16\u001b[39m     prompt,\n\u001b[32m     17\u001b[39m     response_format=Book\n\u001b[32m     18\u001b[39m )\n",
      "\u001b[31mLMStudioWebsocketError\u001b[39m: \n    LM Studio is not reachable at ws://localhost:1234/llm (due to httpx.ConnectError: All connection attempts failed).\n    Is LM Studio running?"
     ]
    }
   ],
   "source": [
    "# Structured output\n",
    "import lmstudio as lms\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Book(BaseModel):\n",
    "    title: str\n",
    "    author: str\n",
    "    year: int\n",
    "    rating: float\n",
    "\n",
    "lmstudio_llm = lms.llm() # Gets the currrent loaded model\n",
    "\n",
    "prompt = \"Tell me about the book 'The Great Gatsby\"\n",
    "\n",
    "response = lmstudio_llm.respond(\n",
    "    prompt,\n",
    "    response_format=Book\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN found in environment variables\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get Hugging Face token from environment variable\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "if hf_token:\n",
    "    print(\"HF_TOKEN found in environment variables\")\n",
    "else:\n",
    "    raise ValueError(\"HF_TOKEN not found in environment variables. Please add it to your .env file\")\n",
    "\n",
    "# Login to Hugging Face\n",
    "import huggingface_hub\n",
    "# huggingface_hub.login(token=hf_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text=' I hope you\\'re having a fantastic day!\\n\\nToday, I wanted to share with you a simple yet powerful JavaScript function that can help you to check if an object is an instance of another object.\\n\\nLet\\'s get started!\\n\\n**The Function:**\\n```javascript\\nfunction isInstanceOf(obj, target) {\\n  return Object.getPrototypeOf(obj) === target.prototype;\\n}\\n```\\n**How it works:**\\n\\nThe `isInstanceOf` function takes two arguments: `obj` (the object you want to check) and `target` (the object you want to check against).\\n\\nThe function uses the `Object.getPrototypeOf()` method to get the prototype of the `obj` object. Then, it compares the result with the `prototype` property of the `target` object using the `===` operator.\\n\\nIf the prototype of `obj` is equal to the `prototype` property of `target`, it means that `obj` is an instance of `target`, and the function returns `true`. Otherwise, it returns `false`.\\n\\n**Example usage:**\\n```javascript\\nclass Person {\\n  constructor(name) {\\n    this.name = name;\\n  }\\n}\\n\\nconst john = new Person(\"John\");\\n\\nconsole.log(isInstanceOf(john, Person)); // Output: true\\nconsole.log(is', additional_kwargs={}, raw=None, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# model_name = \"Qwen/Qwen2.5-Coder-32B-Instruct\" # This is if you want to use a model from huggingface\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "    model_name=model_name,\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000,\n",
    "    token=os.environ.get(\"HF_TOKEN\"),\n",
    ")\n",
    "\n",
    "\n",
    "llm.complete(\"Hello, how are you?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key stages in RAG pipeline\n",
    "1. Loading\n",
    "2. Idexing\n",
    "3. Storing\n",
    "4. Querying\n",
    "5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Directory ./documents does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m load_dotenv()\n\u001b[32m      7\u001b[39m directory_path = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mDOCUMENTS_DIR\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m reader = \u001b[43mSimpleDirectoryReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdirectory_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m documents = reader.load_data()\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m documents\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages/llama_index/core/readers/file/base.py:288\u001b[39m, in \u001b[36mSimpleDirectoryReader.__init__\u001b[39m\u001b[34m(self, input_dir, input_files, exclude, exclude_hidden, exclude_empty, errors, recursive, encoding, filename_as_id, required_exts, file_extractor, num_files_limit, file_metadata, raise_on_error, fs)\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m input_dir:\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fs.isdir(input_dir):\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not exist.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    289\u001b[39m     \u001b[38;5;28mself\u001b[39m.input_dir = _Path(input_dir)\n\u001b[32m    290\u001b[39m     \u001b[38;5;28mself\u001b[39m.exclude = exclude\n",
      "\u001b[31mValueError\u001b[39m: Directory ./documents does not exist."
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from load_dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "directory_path = os.environ.get(\"DOCUMENTS_DIR\")\n",
    "\n",
    "reader = SimpleDirectoryReader(input_dir=directory_path)\n",
    "documents = reader.load_data()\n",
    "print(f\"Found {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing and Node Creation\n",
    "\n",
    "After loading our documents, we need to break them into smaller pieces called Node objects. A Node is just a chunk of text from the original document that's easier for the AI to work with, while it still has references to the original Document object.\n",
    "\n",
    "The IngestionPipeline helps us create these nodes through two key transformations:\n",
    "\n",
    "1. **SentenceSplitter**: Breaks down documents into manageable chunks by splitting them at natural sentence boundaries.\n",
    "2. **HuggingFaceEmbedding**: Converts each chunk into numerical embeddings - vector representations that capture the semantic meaning in a way AI can process efficiently.\n",
    "\n",
    "This process helps us organise our documents in a way that's more useful for searching and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 0\n",
    "# embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedding_model = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "# Create a pipeline with transformations\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap),\n",
    "        HuggingFaceEmbedding(model_name=embedding_model)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply the pipeline to our documents\n",
    "nodes = await pipeline.arun(documents=[Document.example()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing and indexing documents\n",
    "After creating our Node objects we need to index them to make them searchable, but before we can do that, we need a place to store our data.\n",
    "\n",
    "Since we are using an ingestion pipeline, we can directly attach a vector store to the pipeline to populate it. In this case, we will use Chroma to store our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-vector-stores-chroma in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (0.4.1)\n",
      "Requirement already satisfied: chromadb>=0.5.17 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from llama-index-vector-stores-chroma) (0.6.3)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from llama-index-vector-stores-chroma) (0.12.27)\n",
      "Requirement already satisfied: build>=1.0.3 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.11.1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.115.12)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.34.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.2.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.23.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (4.13.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.31.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.21.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.71.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.15.2)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (32.0.1)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (9.0.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.10.16)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (14.0.0)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (2.0.40)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (3.11.14)\n",
      "Requirement already satisfied: banks<3.0.0,>=2.0.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (2.1.0)\n",
      "Requirement already satisfied: dataclasses-json in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (2025.3.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (3.9.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (11.1.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (2.32.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (0.9.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.17.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.18.3)\n",
      "Requirement already satisfied: griffe in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.7.1)\n",
      "Requirement already satisfied: jinja2 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (3.1.6)\n",
      "Requirement already satisfied: packaging>=19.1 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from build>=1.0.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (24.2)\n",
      "Requirement already satisfied: pyproject_hooks in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from build>=1.0.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.2.0)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from fastapi>=0.95.2->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.46.1)\n",
      "Requirement already satisfied: anyio in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (4.9.0)\n",
      "Requirement already satisfied: certifi in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.0.7)\n",
      "Requirement already satisfied: idna in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.3.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.9)\n",
      "Requirement already satisfied: click in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (2024.11.6)\n",
      "Requirement already satisfied: coloredlogs in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb>=0.5.17->llama-index-vector-stores-chroma) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb>=0.5.17->llama-index-vector-stores-chroma) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb>=0.5.17->llama-index-vector-stores-chroma) (5.29.4)\n",
      "Requirement already satisfied: sympy in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.13.1)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (8.6.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.69.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.31.1 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.31.1 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.52b1 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.52b1 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.52b1 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.52b1)\n",
      "Requirement already satisfied: asgiref~=3.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from opentelemetry-instrumentation-asgi==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from pydantic>=1.9->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from pydantic>=1.9->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from pydantic>=1.9->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (3.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from rich>=10.11.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from rich>=10.11.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.19.1)\n",
      "Requirement already satisfied: greenlet>=1 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (3.1.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from tokenizers>=0.13.2->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.29.3)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from typer>=0.9.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.5.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.0.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.1.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.0.4)\n",
      "Requirement already satisfied: websockets>=10.4 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (15.0.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (3.26.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (4.9)\n",
      "Requirement already satisfied: filelock in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.18.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.1.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from anyio->httpx>=0.27.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=0.5.17->llama-index-vector-stores-chroma) (10.0)\n",
      "Requirement already satisfied: colorama>=0.4 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from griffe->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from jinja2->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-vector-stores-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from load_dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Initialize ChromaDB\n",
    "chroma_collection_name = \"rag_collection\"\n",
    "db = chromadb.PersistentClient(path=os.environ.get(\"CHROMA_DB_PATH\"))\n",
    "chroma_collection = db.get_or_create_collection(name=chroma_collection_name)\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# Define the pipeline\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 0\n",
    "embedding_model = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap),\n",
    "        HuggingFaceEmbedding(model_name=embedding_model)\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where vector embeddings come in - by embedding both the query and nodes in the same vector space, we can find relevant matches. The VectorStoreIndex handles this for us, using the same embedding model we used during ingestion to ensure consistency.\n",
    "\n",
    "Let’s see how to create this index from our vector store and embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model name: BAAI/bge-small-en-v1.5\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from load_dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "embedding_model_name = os.environ.get(\"EMBEDDING_MODEL\")\n",
    "if embedding_model_name:\n",
    "    print(f\"Embedding model name: {embedding_model_name}\")\n",
    "else:\n",
    "    raise ValueError(\"Embedding model name not found in environment variables. Please add it to your .env file\")\n",
    "\n",
    "embedding_model = HuggingFaceEmbedding(model_name=embedding_model_name)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store, embed_model=embedding_model)\n",
    "\n",
    "# # Querying the index\n",
    "# query_engine = index.as_query_engine()\n",
    "# response = query_engine.query(\"What is the main idea of the document?\")\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huggingface model name: Qwen/Qwen2.5-Coder-32B-Instruct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(response='Empty Response', source_nodes=[], metadata=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets make a query to the index\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from load_dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "huggingface_model_name = os.environ.get(\"HUGGINGFACE_MODEL\")\n",
    "if huggingface_model_name:\n",
    "    print(f\"Huggingface model name: {huggingface_model_name}\")\n",
    "else:\n",
    "    raise ValueError(\"Huggingface model name not found in environment variables. Please add it to your .env file\")\n",
    "\n",
    "huggingface_llm = HuggingFaceInferenceAPI(model_name=huggingface_model_name)\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=huggingface_llm,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")\n",
    "\n",
    "query_engine.query(\"What is the main idea of the document?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
