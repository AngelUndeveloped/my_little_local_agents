{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama Index Framework\n",
    "This notebook is dedicated to learning about the Llama Index Framework for creating Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to setup your environment currectly \n",
    "# For this project I used python 3.12.3 \n",
    "\n",
    "# Since we are using huggingface models, we need to install the huggingface_hub library\n",
    "# %pip install -q llama-index-llms-huggingface-api llama-index-embeddings-huggingface\n",
    "# %pip install -q lmstudio\n",
    "# %pip install -q load-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded model: DownloadedLlm(model_key='gemma-3-4b-it', display_name='Gemma 3 4B Instruct', architecture='gemma3', vision=True)\n",
      "Downloaded model: DownloadedLlm(model_key='gemma-3-1b-it', display_name='Gemma 3 1B Instruct', architecture='gemma3', vision=False)\n",
      "Downloaded model: DownloadedEmbeddingModel(model_key='text-embedding-nomic-embed-text-v1.5', display_name='Nomic Embed Text v1.5', architecture='nomic-bert')\n",
      "Downloaded model: DownloadedLlm(model_key='gemma-3-12b-it', display_name='Gemma 3 12B Instruct', architecture='gemma3', vision=True)\n",
      "LLM: DownloadedLlm(model_key='gemma-3-4b-it', display_name='Gemma 3 4B Instruct', architecture='gemma3', vision=True)\n",
      "LLM: DownloadedLlm(model_key='gemma-3-1b-it', display_name='Gemma 3 1B Instruct', architecture='gemma3', vision=False)\n",
      "LLM: DownloadedLlm(model_key='gemma-3-12b-it', display_name='Gemma 3 12B Instruct', architecture='gemma3', vision=True)\n",
      "Embedding: DownloadedEmbeddingModel(model_key='text-embedding-nomic-embed-text-v1.5', display_name='Nomic Embed Text v1.5', architecture='nomic-bert')\n"
     ]
    }
   ],
   "source": [
    "# Test out llm studio sdk\n",
    "import lmstudio as lms\n",
    "\n",
    "#List all model avialable locally\n",
    "lmstudio_downloaded_models = lms.list_downloaded_models()\n",
    "for model in lmstudio_downloaded_models:\n",
    "    print(f\"Downloaded model: {model}\")\n",
    "    \n",
    "# List LLM's only\n",
    "lmstudio_llms_only = lms.list_downloaded_models(\"llm\")\n",
    "for llm in lmstudio_llms_only:\n",
    "    print(f\"LLM: {llm}\")\n",
    "    \n",
    "# List embeddings only\n",
    "lmstudio_embeddings_only = lms.list_downloaded_models(\"embedding\")\n",
    "for embedding in lmstudio_embeddings_only:\n",
    "    print(f\"Embedding: {embedding}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmstudio as lms\n",
    "\n",
    "lmstudio_llm = 78 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"author\": \"F. Scott Fitzgerald\",\n",
      "  \"rating\": 4.5,\n",
      "  \"title\": \"The Great Gatsby\",\n",
      "  \"year\": 1925\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Structured output\n",
    "import lmstudio as lms\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Book(BaseModel):\n",
    "    title: str\n",
    "    author: str\n",
    "    year: int\n",
    "    rating: float\n",
    "\n",
    "lmstudio_llm = lms.llm() # Gets the currrent loaded model\n",
    "\n",
    "prompt = \"Tell me about the book 'The Great Gatsby\"\n",
    "\n",
    "response = lmstudio_llm.respond(\n",
    "    prompt,\n",
    "    response_format=Book\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN found in environment variables\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get Hugging Face token from environment variable\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "if hf_token:\n",
    "    print(\"HF_TOKEN found in environment variables\")\n",
    "else:\n",
    "    raise ValueError(\"HF_TOKEN not found in environment variables. Please add it to your .env file\")\n",
    "\n",
    "# Login to Hugging Face\n",
    "import huggingface_hub\n",
    "# huggingface_hub.login(token=hf_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# model_name = \"Qwen/Qwen2.5-Coder-32B-Instruct\" # This is if you want to use a model from huggingface\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "    model_name=model_name,\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000,\n",
    "    token=os.environ.get(\"HF_TOKEN\"),\n",
    ")\n",
    "\n",
    "\n",
    "llm.complete(\"Hello, how are you?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key stages in RAG pipeline\n",
    "1. Loading\n",
    "2. Idexing\n",
    "3. Storing\n",
    "4. Querying\n",
    "5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory path: ~/Projects/ai_projects/my_little_local_agents/documents/system-design-primer\n",
      "Found 9 documents\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from load_dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "directory_path = os.environ.get(\"DOCUMENTS_DIR\")\n",
    "if directory_path:\n",
    "    print(f\"Directory path: {directory_path}\")\n",
    "else:\n",
    "    raise ValueError(\"Directory path not found in environment variables. Please add it to your .env file\")\n",
    "\n",
    "reader = SimpleDirectoryReader(input_dir=directory_path)\n",
    "documents = reader.load_data()\n",
    "print(f\"Found {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing and Node Creation\n",
    "\n",
    "After loading our documents, we need to break them into smaller pieces called Node objects. A Node is just a chunk of text from the original document that's easier for the AI to work with, while it still has references to the original Document object.\n",
    "\n",
    "The IngestionPipeline helps us create these nodes through two key transformations:\n",
    "\n",
    "1. **SentenceSplitter**: Breaks down documents into manageable chunks by splitting them at natural sentence boundaries.\n",
    "2. **HuggingFaceEmbedding**: Converts each chunk into numerical embeddings - vector representations that capture the semantic meaning in a way AI can process efficiently.\n",
    "\n",
    "This process helps us organise our documents in a way that's more useful for searching and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yair/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 0\n",
    "# embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedding_model = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "# Create a pipeline with transformations\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap),\n",
    "        HuggingFaceEmbedding(model_name=embedding_model)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply the pipeline to our documents\n",
    "nodes = await pipeline.arun(documents=[Document.example()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing and indexing documents\n",
    "After creating our Node objects we need to index them to make them searchable, but before we can do that, we need a place to store our data.\n",
    "\n",
    "Since we are using an ingestion pipeline, we can directly attach a vector store to the pipeline to populate it. In this case, we will use Chroma to store our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-vector-stores-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from load_dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Initialize ChromaDB\n",
    "chroma_collection_name = \"rag_collection\"\n",
    "db = chromadb.PersistentClient(path=os.environ.get(\"CHROMA_DB_PATH\"))\n",
    "chroma_collection = db.get_or_create_collection(name=chroma_collection_name)\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# Define the pipeline\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 0\n",
    "embedding_model = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap),\n",
    "        HuggingFaceEmbedding(model_name=embedding_model)\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where vector embeddings come in - by embedding both the query and nodes in the same vector space, we can find relevant matches. The VectorStoreIndex handles this for us, using the same embedding model we used during ingestion to ensure consistency.\n",
    "\n",
    "Let’s see how to create this index from our vector store and embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model name: BAAI/bge-small-en-v1.5\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from load_dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "embedding_model_name = os.environ.get(\"EMBEDDING_MODEL\")\n",
    "if embedding_model_name:\n",
    "    print(f\"Embedding model name: {embedding_model_name}\")\n",
    "else:\n",
    "    raise ValueError(\"Embedding model name not found in environment variables. Please add it to your .env file\")\n",
    "\n",
    "embedding_model = HuggingFaceEmbedding(model_name=embedding_model_name)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store, embed_model=embedding_model)\n",
    "\n",
    "# # Querying the index\n",
    "# query_engine = index.as_query_engine()\n",
    "# response = query_engine.query(\"What is the main idea of the document?\")\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huggingface model name: Qwen/Qwen2.5-Coder-32B-Instruct\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlmstudio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlms\u001b[39;00m\n\u001b[32m     19\u001b[39m lmstudio_llm = lms.llm()\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m query_engine = \u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_query_engine\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# llm=huggingface_llm,\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlmstudio_llm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtree_summarize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m response = query_engine.query(\u001b[33m\"\u001b[39m\u001b[33mWhat is the meaning of life?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py:376\u001b[39m, in \u001b[36mBaseIndex.as_query_engine\u001b[39m\u001b[34m(self, llm, **kwargs)\u001b[39m\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquery_engine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretriever_query_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    371\u001b[39m     RetrieverQueryEngine,\n\u001b[32m    372\u001b[39m )\n\u001b[32m    374\u001b[39m retriever = \u001b[38;5;28mself\u001b[39m.as_retriever(**kwargs)\n\u001b[32m    375\u001b[39m llm = (\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     \u001b[43mresolve_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_callback_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m llm\n\u001b[32m    378\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m Settings.llm\n\u001b[32m    379\u001b[39m )\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m RetrieverQueryEngine.from_args(\n\u001b[32m    382\u001b[39m     retriever,\n\u001b[32m    383\u001b[39m     llm=llm,\n\u001b[32m    384\u001b[39m     **kwargs,\n\u001b[32m    385\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages/llama_index/core/llms/utils.py:102\u001b[39m, in \u001b[36mresolve_llm\u001b[39m\u001b[34m(llm, callback_manager)\u001b[39m\n\u001b[32m     99\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLLM is explicitly disabled. Using MockLLM.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    100\u001b[39m     llm = MockLLM()\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm, LLM)\n\u001b[32m    104\u001b[39m llm.callback_manager = callback_manager \u001b[38;5;129;01mor\u001b[39;00m Settings.callback_manager\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m llm\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Lets make a query to the index\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from load_dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "huggingface_model_name = os.environ.get(\"HUGGINGFACE_MODEL\")\n",
    "if huggingface_model_name:\n",
    "    print(f\"Huggingface model name: {huggingface_model_name}\")\n",
    "else:\n",
    "    raise ValueError(\"Huggingface model name not found in environment variables. Please add it to your .env file\")\n",
    "\n",
    "huggingface_llm = HuggingFaceInferenceAPI(model_name=huggingface_model_name)\n",
    "\n",
    "# using LM Studio\n",
    "import lmstudio as lms\n",
    "\n",
    "lmstudio_llm = lms.llm()\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    # llm=huggingface_llm,\n",
    "    llm=lmstudio_llm,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What is the meaning of life?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
