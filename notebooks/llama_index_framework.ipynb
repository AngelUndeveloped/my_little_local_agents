{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama Index Framework\n",
    "This notebook is dedicated to learning about the Llama Index Framework for creating Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to setup your environment currectly \n",
    "# For this project I used python 3.12.3 \n",
    "\n",
    "# Since we are using huggingface models, we need to install the huggingface_hub library\n",
    "# %pip install -q llama-index-llms-huggingface-api llama-index-embeddings-huggingface\n",
    "# %pip install -q lmstudio\n",
    "# %pip install -q load-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out llm studio sdk\n",
    "import lmstudio as lms\n",
    "\n",
    "#List all model avialable locally\n",
    "lmstudio_downloaded_models = lms.list_downloaded_models()\n",
    "for model in lmstudio_downloaded_models:\n",
    "    print(f\"Downloaded model: {model}\")\n",
    "    \n",
    "# List LLM's only\n",
    "lmstudio_llms_only = lms.list_downloaded_models(\"llm\")\n",
    "for llm in lmstudio_llms_only:\n",
    "    print(f\"LLM: {llm}\")\n",
    "    \n",
    "# List embeddings only\n",
    "lmstudio_embeddings_only = lms.list_downloaded_models(\"embedding\")\n",
    "for embedding in lmstudio_embeddings_only:\n",
    "    print(f\"Embedding: {embedding}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmstudio as lms\n",
    "\n",
    "lmstudio_llm = 78 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured output\n",
    "import lmstudio as lms\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Book(BaseModel):\n",
    "    title: str\n",
    "    author: str\n",
    "    year: int\n",
    "    rating: float\n",
    "\n",
    "lmstudio_llm = lms.llm() # Gets the currrent loaded model\n",
    "\n",
    "prompt = \"Tell me about the book 'The Great Gatsby\"\n",
    "\n",
    "response = lmstudio_llm.respond(\n",
    "    prompt,\n",
    "    response_format=Book\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get Hugging Face token from environment variable\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "if hf_token:\n",
    "    print(\"HF_TOKEN found in environment variables\")\n",
    "else:\n",
    "    raise ValueError(\"HF_TOKEN not found in environment variables. Please add it to your .env file\")\n",
    "\n",
    "# Login to Hugging Face\n",
    "import huggingface_hub\n",
    "# huggingface_hub.login(token=hf_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# model_name = \"Qwen/Qwen2.5-Coder-32B-Instruct\" # This is if you want to use a model from huggingface\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "    model_name=model_name,\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000,\n",
    "    token=os.environ.get(\"HF_TOKEN\"),\n",
    ")\n",
    "\n",
    "\n",
    "llm.complete(\"Hello, how are you?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key stages in RAG pipeline\n",
    "1. Loading\n",
    "2. Idexing\n",
    "3. Storing\n",
    "4. Querying\n",
    "5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from load_dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "directory_path = os.environ.get(\"DOCUMENTS_DIR\")\n",
    "if directory_path:\n",
    "    print(f\"Directory path: {directory_path}\")\n",
    "else:\n",
    "    raise ValueError(\"Directory path not found in environment variables. Please add it to your .env file\")\n",
    "\n",
    "reader = SimpleDirectoryReader(input_dir=directory_path)\n",
    "documents = reader.load_data()\n",
    "print(f\"Found {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing and Node Creation\n",
    "\n",
    "After loading our documents, we need to break them into smaller pieces called Node objects. A Node is just a chunk of text from the original document that's easier for the AI to work with, while it still has references to the original Document object.\n",
    "\n",
    "The IngestionPipeline helps us create these nodes through two key transformations:\n",
    "\n",
    "1. **SentenceSplitter**: Breaks down documents into manageable chunks by splitting them at natural sentence boundaries.\n",
    "2. **HuggingFaceEmbedding**: Converts each chunk into numerical embeddings - vector representations that capture the semantic meaning in a way AI can process efficiently.\n",
    "\n",
    "This process helps us organise our documents in a way that's more useful for searching and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 0\n",
    "# embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedding_model = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "# Create a pipeline with transformations\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap),\n",
    "        HuggingFaceEmbedding(model_name=embedding_model)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply the pipeline to our documents\n",
    "nodes = await pipeline.arun(documents=[Document.example()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing and indexing documents\n",
    "After creating our Node objects we need to index them to make them searchable, but before we can do that, we need a place to store our data.\n",
    "\n",
    "Since we are using an ingestion pipeline, we can directly attach a vector store to the pipeline to populate it. In this case, we will use Chroma to store our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-vector-stores-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from load_dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "# Initialize ChromaDB\n",
    "chroma_collection_name = \"rag_collection\"\n",
    "db = chromadb.PersistentClient(path=os.environ.get(\"CHROMA_DB_PATH\"))\n",
    "chroma_collection = db.get_or_create_collection(name=chroma_collection_name)\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# Define the pipeline\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 0\n",
    "embedding_model = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap),\n",
    "        HuggingFaceEmbedding(model_name=embedding_model)\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where vector embeddings come in - by embedding both the query and nodes in the same vector space, we can find relevant matches. The VectorStoreIndex handles this for us, using the same embedding model we used during ingestion to ensure consistency.\n",
    "\n",
    "Letâ€™s see how to create this index from our vector store and embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from load_dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "embedding_model_name = os.environ.get(\"EMBEDDING_MODEL\")\n",
    "if embedding_model_name:\n",
    "    print(f\"Embedding model name: {embedding_model_name}\")\n",
    "else:\n",
    "    raise ValueError(\"Embedding model name not found in environment variables. Please add it to your .env file\")\n",
    "\n",
    "embedding_model = HuggingFaceEmbedding(model_name=embedding_model_name)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store, embed_model=embedding_model)\n",
    "\n",
    "# # Querying the index\n",
    "# query_engine = index.as_query_engine()\n",
    "# response = query_engine.query(\"What is the main idea of the document?\")\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make a query to the index\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from load_dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "huggingface_model_name = os.environ.get(\"HUGGINGFACE_MODEL\")\n",
    "if huggingface_model_name:\n",
    "    print(f\"Huggingface model name: {huggingface_model_name}\")\n",
    "else:\n",
    "    raise ValueError(\"Huggingface model name not found in environment variables. Please add it to your .env file\")\n",
    "\n",
    "huggingface_llm = HuggingFaceInferenceAPI(model_name=huggingface_model_name)\n",
    "\n",
    "# using LM Studio\n",
    "import lmstudio as lms\n",
    "\n",
    "lmstudio_llm = lms.llm()\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=huggingface_llm,\n",
    "    # llm=lmstudio_llm,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What is the meaning of life?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "\n",
    "# model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\"\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "llm = HuggingFaceInferenceAPI(model_name=model_name)\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")\n",
    "query_engine.query(\"What is the meaning of life?\")\n",
    "# The meaning of life is 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "import lmstudio as lms\n",
    "\n",
    "# llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "llm = HuggingFaceInferenceAPI(model_name=model_name)\n",
    "# llm = lms.llm()\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")\n",
    "\n",
    "# query index\n",
    "evaluator = FaithfulnessEvaluator(llm=llm)\n",
    "\n",
    "response = query_engine.query(\"When was the american civil war?\")\n",
    "\n",
    "eval_result = evaluator.evaluate_response(response=response)\n",
    "eval_result.passing\n",
    "\n",
    "# evaluate response\n",
    "evaluator.evaluate(response, query_engine.query_node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Now we can run the evaluation\n",
    "evaluator = FaithfulnessEvaluator(llm=llm)\n",
    "response = query_engine.query(\"When was the american civil war?\")\n",
    "eval_result = evaluator.evaluate_response(response=response)\n",
    "print(f\"Evaluation passed: {eval_result.passing}\")\n",
    "print(f\"Evaluation score: {eval_result.score}\")\n",
    "print(f\"Evaluation feedback: {eval_result.feedback}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_index\n",
    "import os\n",
    "\n",
    "PHOENIX_API_KEY = \"<PHOENIX_API_KEY>\"\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\n",
    "llama_index.core.set_global_handler(\n",
    "    \"arize_phoenix\",\n",
    "    endpoint=\"https://llamatrace.com/v1/traces\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index datasets llama-index-callbacks-arize-phoenix llama-index-vector-stores-chroma llama-index-llms-huggingface-api -U -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Login to Hugging Face\n",
    "import huggingface_hub\n",
    "from load_dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "huggingface_hub.login(token=os.environ.get(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting weather for New York\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ToolOutput(content='The weather in New York is sunny', tool_name='my_weather_tool', raw_input={'args': ('New York',), 'kwargs': {}}, raw_output='The weather in New York is sunny', is_error=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Useful for getting the weather for a given location.\"\"\"\n",
    "    print(f\"Getting weather for {location}\")\n",
    "    return f\"The weather in {location} is sunny\"\n",
    "\n",
    "def calculate_sum(a: int, b: int) -> int:\n",
    "    \"\"\"Useful for calculating the sum of two numbers.\"\"\"\n",
    "    print(f\"Calculating sum of {a} and {b}\")\n",
    "    return a + b\n",
    "\n",
    "tool = FunctionTool.from_defaults(\n",
    "    get_weather,\n",
    "    name=\"my_weather_tool\",\n",
    "    description=\"Useful for getting the weather for a given location.\",\n",
    ")\n",
    "\n",
    "tool2 = FunctionTool.from_defaults(\n",
    "    calculate_sum,\n",
    "    name=\"my_sum_tool\",\n",
    "    description=\"Useful for calculating the sum of two numbers.\",\n",
    ")\n",
    "\n",
    "tool.call(\"New York\")\n",
    "# tool2.call(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_index.embeddings.huggingface_api'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VectorStoreIndex\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhuggingface_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceInferenceAPI\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membeddings\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhuggingface_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceInferenceAPIEmbedding\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QueryEngineTool\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvector_stores\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchroma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChromaVectorStore\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'llama_index.embeddings.huggingface_api'"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.embeddings.huggingface_api import HuggingFaceInferenceAPIEmbedding\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "db = chromadb.PersistentClient(path=\"./alfred_chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"alfred\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "embed_model = HuggingFaceInferenceAPIEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store, embed_model=embed_model\n",
    ")\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    name=\"some useful name\",\n",
    "    description=\"some useful description\",\n",
    ")\n",
    "await tool.acall(\n",
    "    \"Responds about research on the impact of AI on the future of work and society?\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
