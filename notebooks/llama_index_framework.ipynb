{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama Index Framework\n",
    "This notebook is dedicated to learning about the Llama Index Framework for creating Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to setup your environment currectly \n",
    "# For this project I used python 3.12.3 \n",
    "\n",
    "# Since we are using huggingface models, we need to install the huggingface_hub library\n",
    "# %pip install -q llama-index-llms-huggingface-api llama-index-embeddings-huggingface\n",
    "# %pip install -q lmstudio\n",
    "# %pip install -q load-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded model: DownloadedLlm(model_key='gemma-3-4b-it', display_name='Gemma 3 4B Instruct', architecture='gemma3', vision=True)\n",
      "Downloaded model: DownloadedLlm(model_key='gemma-3-1b-it', display_name='Gemma 3 1B Instruct', architecture='gemma3', vision=False)\n",
      "Downloaded model: DownloadedEmbeddingModel(model_key='text-embedding-nomic-embed-text-v1.5', display_name='Nomic Embed Text v1.5', architecture='nomic-bert')\n",
      "Downloaded model: DownloadedLlm(model_key='gemma-3-12b-it', display_name='Gemma 3 12B Instruct', architecture='gemma3', vision=True)\n",
      "LLM: DownloadedLlm(model_key='gemma-3-4b-it', display_name='Gemma 3 4B Instruct', architecture='gemma3', vision=True)\n",
      "LLM: DownloadedLlm(model_key='gemma-3-1b-it', display_name='Gemma 3 1B Instruct', architecture='gemma3', vision=False)\n",
      "LLM: DownloadedLlm(model_key='gemma-3-12b-it', display_name='Gemma 3 12B Instruct', architecture='gemma3', vision=True)\n",
      "Embedding: DownloadedEmbeddingModel(model_key='text-embedding-nomic-embed-text-v1.5', display_name='Nomic Embed Text v1.5', architecture='nomic-bert')\n"
     ]
    }
   ],
   "source": [
    "# Test out llm studio sdk\n",
    "import lmstudio as lms\n",
    "\n",
    "#List all model avialable locally\n",
    "lmstudio_downloaded_models = lms.list_downloaded_models()\n",
    "for model in lmstudio_downloaded_models:\n",
    "    print(f\"Downloaded model: {model}\")\n",
    "    \n",
    "# List LLM's only\n",
    "lmstudio_llms_only = lms.list_downloaded_models(\"llm\")\n",
    "for llm in lmstudio_llms_only:\n",
    "    print(f\"LLM: {llm}\")\n",
    "    \n",
    "# List embeddings only\n",
    "lmstudio_embeddings_only = lms.list_downloaded_models(\"embedding\")\n",
    "for embedding in lmstudio_embeddings_only:\n",
    "    print(f\"Embedding: {embedding}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmstudio as lms\n",
    "\n",
    "lmstudio_llm = 78 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"author\": \"F. Scott Fitzgerald\",\n",
      "  \"rating\": 4.5,\n",
      "  \"title\": \"The Great Gatsby\",\n",
      "  \"year\": 1925\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Structured output\n",
    "import lmstudio as lms\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Book(BaseModel):\n",
    "    title: str\n",
    "    author: str\n",
    "    year: int\n",
    "    rating: float\n",
    "\n",
    "lmstudio_llm = lms.llm() # Gets the currrent loaded model\n",
    "\n",
    "prompt = \"Tell me about the book 'The Great Gatsby\"\n",
    "\n",
    "response = lmstudio_llm.respond(\n",
    "    prompt,\n",
    "    response_format=Book\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN found in environment variables\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get Hugging Face token from environment variable\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "if hf_token:\n",
    "    print(\"HF_TOKEN found in environment variables\")\n",
    "else:\n",
    "    raise ValueError(\"HF_TOKEN not found in environment variables. Please add it to your .env file\")\n",
    "\n",
    "# Login to Hugging Face\n",
    "import huggingface_hub\n",
    "# huggingface_hub.login(token=hf_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text=\" I hope you're doing well. I just wanted to let you know that I've been thinking about you a lot lately. I've been going through some tough times, and I often think about our conversations and the advice you gave me. It really helped me a lot, and I appreciate your support. I wish I could be there in person to thank you, but I hope you know that you're always in my thoughts and prayers.\\nI also wanted to share some good news with you. I recently got a new job, and it's been a great experience so far. I'm learning a lot and meeting new people every day. It's been a big adjustment, but I'm excited to see where this new chapter in my life takes me.\\nI hope you're doing well too. I'd love to catch up and hear about what's been going on in your life. Please let me know if you're free to talk sometime. I'd appreciate it. Take care, and thank you again for everything.\\nSincerely,\\n[Your Name] Continue reading â†’\\nTagged: business etiquette, career advice, email etiquette, job search, networking, professional development, professionalism, respect, thank you notes\\nHow to Write a Professional Email for a Job Application\\nWhen applying\", additional_kwargs={}, raw=None, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# model_name = \"Qwen/Qwen2.5-Coder-32B-Instruct\" # This is if you want to use a model from huggingface\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "    model_name=model_name,\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000,\n",
    "    token=os.environ.get(\"HF_TOKEN\"),\n",
    ")\n",
    "\n",
    "\n",
    "llm.complete(\"Hello, how are you?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key stages in RAG pipeline\n",
    "1. Loading\n",
    "2. Idexing\n",
    "3. Storing\n",
    "4. Querying\n",
    "5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory path: ~/Projects/ai_projects/my_little_local_agents/documents\n",
      "Found 8 documents\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from load_dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "directory_path = os.environ.get(\"DOCUMENTS_DIR\")\n",
    "if directory_path:\n",
    "    print(f\"Directory path: {directory_path}\")\n",
    "else:\n",
    "    raise ValueError(\"Directory path not found in environment variables. Please add it to your .env file\")\n",
    "\n",
    "reader = SimpleDirectoryReader(input_dir=directory_path)\n",
    "documents = reader.load_data()\n",
    "print(f\"Found {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing and Node Creation\n",
    "\n",
    "After loading our documents, we need to break them into smaller pieces called Node objects. A Node is just a chunk of text from the original document that's easier for the AI to work with, while it still has references to the original Document object.\n",
    "\n",
    "The IngestionPipeline helps us create these nodes through two key transformations:\n",
    "\n",
    "1. **SentenceSplitter**: Breaks down documents into manageable chunks by splitting them at natural sentence boundaries.\n",
    "2. **HuggingFaceEmbedding**: Converts each chunk into numerical embeddings - vector representations that capture the semantic meaning in a way AI can process efficiently.\n",
    "\n",
    "This process helps us organise our documents in a way that's more useful for searching and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 0\n",
    "# embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedding_model = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "# Create a pipeline with transformations\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap),\n",
    "        HuggingFaceEmbedding(model_name=embedding_model)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply the pipeline to our documents\n",
    "nodes = await pipeline.arun(documents=[Document.example()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing and indexing documents\n",
    "After creating our Node objects we need to index them to make them searchable, but before we can do that, we need a place to store our data.\n",
    "\n",
    "Since we are using an ingestion pipeline, we can directly attach a vector store to the pipeline to populate it. In this case, we will use Chroma to store our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-vector-stores-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from load_dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "# Initialize ChromaDB\n",
    "chroma_collection_name = \"rag_collection\"\n",
    "db = chromadb.PersistentClient(path=os.environ.get(\"CHROMA_DB_PATH\"))\n",
    "chroma_collection = db.get_or_create_collection(name=chroma_collection_name)\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# Define the pipeline\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 0\n",
    "embedding_model = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap),\n",
    "        HuggingFaceEmbedding(model_name=embedding_model)\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where vector embeddings come in - by embedding both the query and nodes in the same vector space, we can find relevant matches. The VectorStoreIndex handles this for us, using the same embedding model we used during ingestion to ensure consistency.\n",
    "\n",
    "Letâ€™s see how to create this index from our vector store and embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model name: BAAI/bge-small-en-v1.5\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from load_dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "embedding_model_name = os.environ.get(\"EMBEDDING_MODEL\")\n",
    "if embedding_model_name:\n",
    "    print(f\"Embedding model name: {embedding_model_name}\")\n",
    "else:\n",
    "    raise ValueError(\"Embedding model name not found in environment variables. Please add it to your .env file\")\n",
    "\n",
    "embedding_model = HuggingFaceEmbedding(model_name=embedding_model_name)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store, embed_model=embedding_model)\n",
    "\n",
    "# # Querying the index\n",
    "# query_engine = index.as_query_engine()\n",
    "# response = query_engine.query(\"What is the main idea of the document?\")\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huggingface model name: Qwen/Qwen2.5-Coder-32B-Instruct\n",
      "Empty Response\n"
     ]
    }
   ],
   "source": [
    "# Lets make a query to the index\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from load_dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "huggingface_model_name = os.environ.get(\"HUGGINGFACE_MODEL\")\n",
    "if huggingface_model_name:\n",
    "    print(f\"Huggingface model name: {huggingface_model_name}\")\n",
    "else:\n",
    "    raise ValueError(\"Huggingface model name not found in environment variables. Please add it to your .env file\")\n",
    "\n",
    "huggingface_llm = HuggingFaceInferenceAPI(model_name=huggingface_model_name)\n",
    "\n",
    "# using LM Studio\n",
    "import lmstudio as lms\n",
    "\n",
    "lmstudio_llm = lms.llm()\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=huggingface_llm,\n",
    "    # llm=lmstudio_llm,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What is the meaning of life?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='Empty Response', source_nodes=[], metadata=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "\n",
    "# model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\"\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "llm = HuggingFaceInferenceAPI(model_name=model_name)\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")\n",
    "query_engine.query(\"What is the meaning of life?\")\n",
    "# The meaning of life is 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Detected nested async. Please use nest_asyncio.apply() to allow nested event loops.Or, use async entry methods like `aquery()`, `aretriever`, `achat`, etc.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages/llama_index/core/async_utils.py:33\u001b[39m, in \u001b[36masyncio_run\u001b[39m\u001b[34m(coro)\u001b[39m\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# If we're here, there's an existing loop but it's not running\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoro\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     36\u001b[39m     \u001b[38;5;66;03m# If we can't get the event loop, we're likely in a different thread, or its already running\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/asyncio/base_events.py:663\u001b[39m, in \u001b[36mBaseEventLoop.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    662\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    665\u001b[39m new_task = \u001b[38;5;129;01mnot\u001b[39;00m futures.isfuture(future)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/asyncio/base_events.py:622\u001b[39m, in \u001b[36mBaseEventLoop._check_running\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_running():\n\u001b[32m--> \u001b[39m\u001b[32m622\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mThis event loop is already running\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mRuntimeError\u001b[39m: This event loop is already running",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages/llama_index/core/async_utils.py:38\u001b[39m, in \u001b[36masyncio_run\u001b[39m\u001b[34m(coro)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoro\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/asyncio/runners.py:190\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug, loop_factory)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    189\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    191\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug, loop_factory=loop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n",
      "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     16\u001b[39m evaluator = FaithfulnessEvaluator(llm=llm)\n\u001b[32m     18\u001b[39m response = query_engine.query(\u001b[33m\"\u001b[39m\u001b[33mWhen was the american civil war?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m eval_result = \u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m eval_result.passing\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# evaluate response\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages/llama_index/core/evaluation/base.py:106\u001b[39m, in \u001b[36mBaseEvaluator.evaluate_response\u001b[39m\u001b[34m(self, query, response, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m     response_str = response.response\n\u001b[32m    104\u001b[39m     contexts = [node.get_content() \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m response.source_nodes]\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages/llama_index/core/evaluation/base.py:64\u001b[39m, in \u001b[36mBaseEvaluator.evaluate\u001b[39m\u001b[34m(self, query, response, contexts, **kwargs)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate\u001b[39m(\n\u001b[32m     52\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     53\u001b[39m     query: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     56\u001b[39m     **kwargs: Any,\n\u001b[32m     57\u001b[39m ) -> EvaluationResult:\n\u001b[32m     58\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Run evaluation with query string, retrieved contexts,\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[33;03m    and generated response string.\u001b[39;00m\n\u001b[32m     60\u001b[39m \n\u001b[32m     61\u001b[39m \u001b[33;03m    Subclasses can override this method to provide custom evaluation logic and\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[33;03m    take in additional arguments.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai_projects/my_little_local_agents/.venv/lib/python3.12/site-packages/llama_index/core/async_utils.py:40\u001b[39m, in \u001b[36masyncio_run\u001b[39m\u001b[34m(coro)\u001b[39m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m asyncio.run(coro)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     41\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mDetected nested async. Please use nest_asyncio.apply() to allow nested event loops.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     42\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOr, use async entry methods like `aquery()`, `aretriever`, `achat`, etc.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     43\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: Detected nested async. Please use nest_asyncio.apply() to allow nested event loops.Or, use async entry methods like `aquery()`, `aretriever`, `achat`, etc."
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "import lmstudio as lms\n",
    "\n",
    "# llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "llm = HuggingFaceInferenceAPI(model_name=model_name)\n",
    "# llm = lms.llm()\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")\n",
    "\n",
    "# query index\n",
    "evaluator = FaithfulnessEvaluator(llm=llm)\n",
    "\n",
    "response = query_engine.query(\"When was the american civil war?\")\n",
    "\n",
    "eval_result = evaluator.evaluate_response(response=response)\n",
    "eval_result.passing\n",
    "\n",
    "# evaluate response\n",
    "evaluator.evaluate(response, query_engine.query_node)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
