{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama Index Framework\n",
    "This notebook is dedicated to learning about the Llama Index Framework for creating Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Make sure to setup your environment currectly \n",
    "# For this project I used python 3.12.3 \n",
    "\n",
    "# Since we are using huggingface models, we need to install the huggingface_hub library\n",
    "%pip install -q llama-index-llms-huggingface-api llama-index-embeddings-huggingface\n",
    "%pip install -q lmstudio\n",
    "%pip install -q load-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded model: DownloadedLlm(model_key='gemma-3-4b-it', display_name='Gemma 3 4B Instruct', architecture='gemma3', vision=True)\n",
      "Downloaded model: DownloadedLlm(model_key='gemma-3-1b-it', display_name='Gemma 3 1B Instruct', architecture='gemma3', vision=False)\n",
      "Downloaded model: DownloadedEmbeddingModel(model_key='text-embedding-nomic-embed-text-v1.5', display_name='Nomic Embed Text v1.5', architecture='nomic-bert')\n",
      "Downloaded model: DownloadedLlm(model_key='gemma-3-12b-it', display_name='Gemma 3 12B Instruct', architecture='gemma3', vision=True)\n",
      "LLM: DownloadedLlm(model_key='gemma-3-4b-it', display_name='Gemma 3 4B Instruct', architecture='gemma3', vision=True)\n",
      "LLM: DownloadedLlm(model_key='gemma-3-1b-it', display_name='Gemma 3 1B Instruct', architecture='gemma3', vision=False)\n",
      "LLM: DownloadedLlm(model_key='gemma-3-12b-it', display_name='Gemma 3 12B Instruct', architecture='gemma3', vision=True)\n",
      "Embedding: DownloadedEmbeddingModel(model_key='text-embedding-nomic-embed-text-v1.5', display_name='Nomic Embed Text v1.5', architecture='nomic-bert')\n"
     ]
    }
   ],
   "source": [
    "# Test out llm studio sdk\n",
    "import lmstudio as lms\n",
    "\n",
    "#List all model avialable locally\n",
    "lmstudio_downloaded_models = lms.list_downloaded_models()\n",
    "for model in lmstudio_downloaded_models:\n",
    "    print(f\"Downloaded model: {model}\")\n",
    "    \n",
    "# List LLM's only\n",
    "lmstudio_llms_only = lms.list_downloaded_models(\"llm\")\n",
    "for llm in lmstudio_llms_only:\n",
    "    print(f\"LLM: {llm}\")\n",
    "    \n",
    "# List embeddings only\n",
    "lmstudio_embeddings_only = lms.list_downloaded_models(\"embedding\")\n",
    "for embedding in lmstudio_embeddings_only:\n",
    "    print(f\"Embedding: {embedding}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmstudio as lms\n",
    "\n",
    "lmstudio_llm = 78 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"author\": \"F. Scott Fitzgerald\",\n",
      "  \"rating\": 4.3,\n",
      "  \"title\": \"The Great Gatsby\",\n",
      "  \"year\": 1925\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Structured output\n",
    "import lmstudio as lms\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Book(BaseModel):\n",
    "    title: str\n",
    "    author: str\n",
    "    year: int\n",
    "    rating: float\n",
    "\n",
    "lmstudio_llm = lms.llm() # Gets the currrent loaded model\n",
    "\n",
    "prompt = \"Tell me about the book 'The Great Gatsby\"\n",
    "\n",
    "response = lmstudio_llm.respond(\n",
    "    prompt,\n",
    "    response_format=Book\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN found in environment variables\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get Hugging Face token from environment variable\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "if hf_token:\n",
    "    print(\"HF_TOKEN found in environment variables\")\n",
    "else:\n",
    "    raise ValueError(\"HF_TOKEN not found in environment variables. Please add it to your .env file\")\n",
    "\n",
    "# Login to Hugging Face\n",
    "import huggingface_hub\n",
    "# huggingface_hub.login(token=hf_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text=' It\\'s great to see you here.\\nI\\'m doing well, thanks for asking. I\\'m excited to be talking to you today.\\n\\nSo, what\\'s been going on in your life lately? Have you been up to any interesting projects or activities?\\n\\nI\\'ve been keeping busy with my usual routine, but I did recently take a trip to a nearby city. It was a lot of fun, and I got to try some new foods and see some new sights.\\n\\nHow about you? Have you been up to anything exciting lately?\\n\\nI\\'m glad you\\'re doing well. It\\'s always great to hear about people\\'s adventures and experiences.\\n\\nWell, I\\'m glad we had a chance to catch up. It was great talking to you, and I hope we can do it again soon. Take care!... more\\n\\nAs the conversation continues, the user is presented with a series of questions and responses that are designed to simulate a natural conversation. This is a simple example of a chatbot, which is a program that uses artificial intelligence to generate a conversation with a human.\\n\\nHere is the code for this chatbot:\\n```\\nimport random\\n\\n# Define a list of responses\\nresponses = [\\n    \"Hello, how are you? It\\'s great to see you here.\",\\n    \"', additional_kwargs={}, raw=None, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# model_name = \"Qwen/Qwen2.5-Coder-32B-Instruct\" # This is if you want to use a model from huggingface\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "    model_name=model_name,\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000,\n",
    "    token=os.environ.get(\"HF_TOKEN\"),\n",
    ")\n",
    "\n",
    "\n",
    "llm.complete(\"Hello, how are you?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key stages in RAG pipeline\n",
    "1. Loading\n",
    "2. Idexing\n",
    "3. Storing\n",
    "4. Querying\n",
    "5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 documents\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from load_dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "directory_path = os.environ.get(\"DOCUMENTS_DIR\")\n",
    "\n",
    "reader = SimpleDirectoryReader(input_dir=directory_path)\n",
    "documents = reader.load_data()\n",
    "print(f\"Found {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
