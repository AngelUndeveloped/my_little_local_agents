{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning your function for Function-calling\n",
    "https://huggingface.co/learn/agents-course/bonus-unit1/introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a typical conversation you would interact in the following way\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"I need help with my order\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'd be happy to help. Could you provide your order number?\"},\n",
    "    {\"role\": \"user\", \"content\": \"It's ORDER-123\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calling brings in a new role for Action and one new role for Observation\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the status of my transaction T1001?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"\",\n",
    "        \"function_call\": {\n",
    "            \"name\": \"retrieve_payment_status\",\n",
    "            \"arguments\": \"{\\\"transaction_id\\\": \\\"T1001\\\"}\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"tool\",\n",
    "        \"name\": \"retrieve_payment_status\",\n",
    "        \"content\": \"{\\\"status\\\": \\\"Paid\\\"}\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Your transaction T1001 has been successfully paid.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning a model for Function-Calling\n",
    "https://huggingface.co/agents-course/notebooks/blob/main/bonus-unit1/bonus-unit1.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\angel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\angel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\angel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\angel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\angel\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\angel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\angel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\angel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\angel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\angel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\angel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\angel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\angel\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\angel\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\angel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\angel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\angel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\angel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install transformers\n",
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q -U bitsandbytes # Efficient memory management and operations on large-scale neural networks\n",
    "# %pip install -q -U peft # fine-tuning models with fewer parameters\n",
    "# %pip install -q -U trl # Reinforcement Learning Libreries for implementing learning algorithms.\n",
    "# %pip install -q -U tensorboardX # Visualize pytorch training results in a TensorBoard\n",
    "# %pip install -q wandb #Weights and Biases tool for tracking experiments, visualizing results, and managing collaberation.\n",
    "# %pip install -q -U python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from enum import Enum  # Enum for creating enumerations\n",
    "from functools import partial  # Partial function application\n",
    "import pandas as pd  # Pandas for data manipulation\n",
    "import torch  # PyTorch for deep learning\n",
    "import json  # JSON module for handling JSON data\n",
    "\n",
    "# Import Hugging Face Transformers and related libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed  # Transformer model, tokenizer, and seed setting\n",
    "from datasets import load_dataset  # Load datasets from Hugging Face's datasets library\n",
    "from trl import SFTConfig, SFTTrainer  # TRL (Transformers Reinforcement Learning) configurations and trainer\n",
    "from peft import LoraConfig, TaskType  # Parameter-efficient fine-tuning (PEFT) configurations\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "from dotenv import load_dotenv  \n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "load_dotenv()  # Load environment variables from the .env file\n",
    "\n",
    "# Retrieve the token from environment variables\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Authenticate hugging face token\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "else:\n",
    "    print(\"Hugging Face token not found.\")\n",
    "\n",
    "seed = 42 # from the guide\n",
    "set_seed(seed)  # Set the seed for reproducibility\n",
    "\n",
    "import os\n",
    "os.environ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"{{ bos_token }}\n",
    "{% if messages[0]['role'] == 'system' %}\n",
    "{{ raise_exception('System role not supported') }}\n",
    "{% endif %}\n",
    "{% for message in messages %}\n",
    "{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}\n",
    "{% endfor %}\n",
    "{% if add_generation_prompt %}\n",
    "{{'<start_of_turn>model\\n'}}\n",
    "{% endif %}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jofthomas/hermes-function-calling-thinking-V1\n"
     ]
    }
   ],
   "source": [
    "# %pip install -q -U tabulate\n",
    "import tabulate \n",
    "model_name = \"google/gemma-2-2b-it\"\n",
    "dataset_name = \"Jofthomas/hermes-function-calling-thinking-V1\"\n",
    "print(dataset_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n",
    "\n",
    "def preprocess(sample):\n",
    "   messages = sample[\"messages\"]\n",
    "   first_message = messages[0] \n",
    "\n",
    "   # Instead of adding a system message, we merge the content into the first use message\n",
    "   if first_message[\"role\"] == \"system\":\n",
    "      system_message_content = first_message[\"content\"]\n",
    "      # Merge system content with the first user message\n",
    "      messages[1][\"content\"] = system_message_content + \"Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\\n\\n\" + messages[1][\"content\"]\n",
    "      # Remove the system message form the conversation\n",
    "      messages.pop(0)\n",
    "\n",
    "   return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False)}\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "dataset = dataset.rename_column(\"conversations\", \"messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3570/3570 [00:00<00:00, 7349.59 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3213\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 357\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(preprocess, remove_columns=\"messages\")\n",
    "dataset = dataset[\"train\"].train_test_split(0.1)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>human\n",
      "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'search_movies', 'description': 'Search for movies based on given criteria', 'parameters': {'type': 'object', 'properties': {'genre': {'type': 'string', 'description': 'The genre of the movie'}, 'year': {'type': 'integer', 'description': 'The year of release'}, 'rating': {'type': 'number', 'description': 'The minimum rating of the movie'}}, 'required': []}}}, {'type': 'function', 'function': {'name': 'convert_currency', 'description': 'Convert currency from one unit to another', 'parameters': {'type': 'object', 'properties': {'amount': {'type': 'number', 'description': 'The amount to convert'}, 'from_currency': {'type': 'string', 'description': 'The current currency unit'}, 'to_currency': {'type': 'string', 'description': 'The desired currency unit'}}, 'required': ['amount', 'from_currency', 'to_currency']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
      "<tool_call>\n",
      "{tool_call}\n",
      "</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\n",
      "\n",
      "I want to watch a movie tonight. Can you suggest me some?<end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "Of course! Could you please tell me your preferred genre, year of release, and minimum rating you would like for the movie?<end_of_turn><eos>\n",
      "<start_of_turn>human\n",
      "I like action movies. I don't mind about the year. But I would prefer a movie with a rating of at least 7.<end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "<think>Alright, let's break down how I arrived at suggesting the next function call. \n",
      "\n",
      "So, the user initially asked for movie suggestions. I responded by asking for their preferences regarding genre, year, and rating. They mentioned they like action movies, don't care about the release year, and want a minimum rating of 7.\n",
      "\n",
      "With this information, I considered the available functions. The `search_movies` function fits perfectly because it allows searching based on genre, year, and rating. The user provided all necessary parameters except the year, which is optional, so I can proceed without it.\n",
      "\n",
      "I decided to call `search_movies` with the genre set to 'action' and the rating set to 7. This setup will fetch movies that match the user's preferences, providing a tailored suggestion. No other function like `convert_currency` was relevant here since the query was solely about movies. \n",
      "\n",
      "So, the next step is to execute the `search_movies` function with the given criteria to find suitable movies for the user.\n",
      "</think><tool_call>\n",
      "{'name': 'search_movies', 'arguments': {'genre': 'action', 'rating': 7}}\n",
      "</tool_call><end_of_turn><eos>\n",
      "<start_of_turn>tool\n",
      "<tool_response>\n",
      "{'movies': [{'title': 'Die Hard', 'year': 1988, 'rating': 8.2}, {'title': 'Mad Max: Fury Road', 'year': 2015, 'rating': 8.1}, {'title': 'The Dark Knight', 'year': 2008, 'rating': 9.0}]}\n",
      "</tool_response><end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "Here are some action movies with a rating of at least 7 that you might enjoy: \"Die Hard\" from 1988 with a rating of 8.2, \"Mad Max: Fury Road\" from 2015 with a rating of 8.1, and \"The Dark Knight\" from 2008 with a rating of 9.0.<end_of_turn><eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][8][\"text\"])\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(tokenizer.pad_token)\n",
    "print(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatmlSpecialTokens(str, Enum):\n",
    "    tools = \"<tools>\"\n",
    "    eotools = \"</tools>\"\n",
    "    think = \"<think>\"\n",
    "    eothink = \"</think>\"\n",
    "    tool_call = \"<tool_call>\"\n",
    "    eottool_call = \"</tool_call>\"\n",
    "    tool_response = \"<tool_response>\"\n",
    "    eotool_response = \"</tool_response>\"\n",
    "    pad_token = \"<pad>\"\n",
    "    eos_token = \"<eos>\"\n",
    "    @classmethod\n",
    "    def list(cls):\n",
    "        return [c.value for c in cls]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    pad_token=ChatmlSpecialTokens.pad_token.value,\n",
    "    additional_special_tokens=ChatmlSpecialTokens.list(),\n",
    ")\n",
    "tokenizer.chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    attn_implmentation='eager',\n",
    "    device_map=\"auto\")\n",
    "print(f\"Number of tokens: {len(tokenizer)}\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(torch.bfloat16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
